#!/usr/bin/env python

import os
import subprocess
import sys
import urlparse

import pydoop.hdfs as phdfs

from pathset import FilePathset

ValidModes = ('default', 'local')

class Uri(object):
  def __init__(self, *args):
    if len(args) == 1 and all(hasattr(args[0], attr) for attr in ('scheme', 'netloc', 'path')):
      self.scheme = args[0].scheme
      self.netloc = args[0].netloc
      self.path = args[0].path
    elif len(args) == 3:
      self.scheme, self.netloc, self.path = args
    else:
      raise ValueError()
    if self.scheme == 'file':
      if self.netloc:
        raise ValueError("Can't specify a netloc with file: scheme")
      if self.path and not self.path.starswith('/'):
        raise ValueError("Must use absolute paths with file: scheme (found %s)" % self.path)
    if self.netloc and not self.scheme:
      raise ValueError("Can't specify a host without an access scheme")

  def geturl(self):
    if self.scheme:
      url = "%s://%s%s" % (self.scheme, self.netloc, self.path)
    else:
      url = self.path
    return url

def usage_error(msg = None):
  if msg:
    print >>sys.stderr, msg
  print >>sys.stderr, os.path.basename(sys.argv[0]), "[ %s ]" % ' | '.join(ValidModes), "DATAPATH", "OUTPUT_PATH"
  sys.exit(1)

def get_default_fs():
  root_ls = phdfs.ls('/')
  if root_ls:
    uri = Uri(urlparse.urlparse(root_ls[0]))
    return uri
  else:
    raise RuntimeError("Could not determine URI of default file system.  It's empty.")

def resolve_datapath(mode, datapath):
  """
  Returns a full URL for datapath
  """
  u = Uri(urlparse.urlparse(datapath))

  if not u.path:
    raise RuntimeError("missing path in data path %s" % datapath)

  if mode == 'default' and not u.scheme: # datapath not specified completely. Assume it's on the default fs
    u = Uri(urlparse.urlparse(phdfs.path.abspath(u.path)))
  elif mode == 'local':
    if u.scheme and u.scheme != 'file':
      raise RuntimeError("Specified local mode but datapath is a URI with scheme %s (expected no scheme or 'file')" % u.scheme)
    # force the 'file' scheme and make the path absolute
    u.scheme = 'file'
    u.netloc = ''
    u.path = os.path.abspath(datapath)
  return u


def get_paths(datapath_uri):
  # simple case:  the path simply exists
  if phdfs.path.exists(datapath_uri.geturl()):
    return [datapath_uri.geturl()]

  # second case:  the path doesn't exist as it is.  It may contain wildcards, so we try
  # listing the datapath with hadoop dfs.  If we were to list with
  # pydoop.hdfs.ls we'd have to implement hadoop wildcards ourselves (perhaps with fnmatch)

  def process(ls_line):
    path = ls_line[(ls_line.rindex(' ') + 1):]
    url = Uri(urlparse.urlparse(path))
    url.scheme = datapath_uri.scheme
    url.netloc = datapath_uri.netloc
    return url.geturl()

  try:
    # run -ls with hadoop dfs the process the output.
    # We drop the first line since it's something like "Found xx items".
    ls_output = subprocess.check_output(['hadoop', 'dfs', '-ls', datapath_uri.geturl()]).rstrip('\n').split('\n')[1:]
    # for each data line, run apply the 'process' function to transform it into a full URI
    return map(process, ls_output)
  except subprocess.CalledProcessError, e:
    print >>sys.stderr, "Could not list datapath %s.  Please check whether it exists" % datapath_uri.geturl()
    sys.exit(1)
  

if __name__ == "__main__":
  if len(sys.argv) != 4:
    usage_error()
  mode, datapath, output_path = sys.argv[1:]

  # test the hadoop configuration
  cmd = ['hadoop', 'dfs', '-ls', '/']
  try:
    subprocess.check_output(cmd)
  except Exception, e:
    print >>sys.stderr, "Error running hadoop program.  Please check your environment (tried %s)" % ' '.join(cmd)
    sys.exit(2)

  if mode not in ValidModes:
    usage_error("mode must be one of %s" % ', '.join(ValidModes))

  uri = resolve_datapath(mode, datapath)
  path_uris = get_paths(uri)
  output_pathset = FilePathset(*path_uris)
  with open(output_path, 'w') as f:
    output_pathset.write(f)
